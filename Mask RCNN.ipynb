{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FaisalT0435/Sistem-Deteksi-Hama-menggunakan-Deep-learning-dan-IoT/blob/main/Mask%20RCNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8iwJ4Siy_cF",
        "outputId": "e2cba94e-61df-471b-fd15-40f3396ab6f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "rpk26q-xH52P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bdc3a7d-76d8-4541-a028-28ed60e93ff5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.8/dist-packages (2.0.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from pycocotools) (1.21.6)\n",
            "Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.8/dist-packages (from pycocotools) (3.2.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=2.1.0->pycocotools) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=2.1.0->pycocotools) (1.4.4)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=2.1.0->pycocotools) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=2.1.0->pycocotools) (0.11.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.1->matplotlib>=2.1.0->pycocotools) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.8/dist-packages (4.4.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from gdown) (4.64.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.8/dist-packages (from gdown) (4.6.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from gdown) (3.9.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.8/dist-packages (from gdown) (2.25.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (1.24.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (2022.12.7)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (1.7.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install pycocotools\n",
        "!pip install gdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "J6W2gZkYH8VR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "wHh3uCVhH_7h"
      },
      "outputs": [],
      "source": [
        "os.chdir('/content/drive/MyDrive/Ta/Mask_RCNN/Dataset')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "c9W2e91TeMt4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95e5edb5-35c2-45c0-b49e-5033b05b5001"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Ta/Mask_RCNN/Dataset\n"
          ]
        }
      ],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "Y9brMriGIZBo"
      },
      "outputs": [],
      "source": [
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "LEARNING_RATE = 1e-5\n",
        "WEIGHT_DECAY = 5e-4\n",
        "BATCH_SIZE = 8\n",
        "NUM_EPOCHS = 1\n",
        "NUM_WORKERS = 4\n",
        "CHECKPOINT_FILE = \"1152KaggleBest.pth.tar\"\n",
        "PIN_MEMORY = True\n",
        "SAVE_MODEL = False\n",
        "LOAD_MODEL = False\n",
        "TRAIN_DIR = 'train'\n",
        "VALID_DIR = 'valid'\n",
        "TEST_DIR = 'test'\n",
        "IMAGE_SIZE = [416,416]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "8zEVtL7QIn3g"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from pycocotools.coco import COCO\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "O1hvuunwIrx6"
      },
      "outputs": [],
      "source": [
        "class CCDataset(Dataset):\n",
        "  def __init__(self, mode = 'train', augmentation=None):\n",
        "    if mode == 'train':\n",
        "      self.dataset_path = TRAIN_DIR\n",
        "      ann_path = os.path.join(TRAIN_DIR, '_annotations.coco.json')\n",
        "    if mode == 'valid':\n",
        "      self.dataset_path = VALID_DIR\n",
        "      ann_path = os.path.join(VALID_DIR, '_annotations.coco.json')\n",
        "    if mode == 'test':\n",
        "      self.dataset_path = TEST_DIR\n",
        "      ann_path = os.path.join(TEST_DIR, '_annotations.coco.json')\n",
        "    \n",
        "    self.coco = COCO(ann_path)\n",
        "    self.cat_ids = self.coco.getCatIds()\n",
        "    self.augmentation=augmentation\n",
        "\n",
        "  def __len__(self):\n",
        "      return len(self.coco.imgs)\n",
        "  \n",
        "  # memamnggil segmentasi \n",
        "  def get_masks(self, index):\n",
        "      ann_ids = self.coco.getAnnIds([index])\n",
        "      anns = self.coco.loadAnns(ann_ids)\n",
        "      masks=[]\n",
        "\n",
        "      for ann in anns:\n",
        "            mask = self.coco.annToMask(ann)\n",
        "            masks.append(mask)\n",
        "\n",
        "      return masks\n",
        "  \n",
        "  # memamnggil bounding box\n",
        "  def get_boxes(self, masks):\n",
        "      num_objs = len(masks)\n",
        "      boxes = []\n",
        "\n",
        "      for i in range(num_objs):\n",
        "          x,y,w,h = cv2.boundingRect(masks[i])\n",
        "          boxes.append([x, y, x+w, y+h])\n",
        "\n",
        "      return np.array(boxes)\n",
        "\n",
        "# memamnggil gambar\n",
        "  def __getitem__(self, index):\n",
        "      # Load image\n",
        "      img_info = self.coco.loadImgs([index])[0]\n",
        "      image = cv2.imread(os.path.join(self.dataset_path,\n",
        "                                    img_info['file_name']))\n",
        "      # memanggil segmentasi sesuai index\n",
        "      masks = self.get_masks(index)\n",
        "\n",
        "      if self.augmentation:\n",
        "         augmented = self.augmentation(image=image, masks=masks)\n",
        "         image, masks = augmented['image'], augmented['masks']\n",
        "\n",
        "      image = image.transpose(2,0,1) / 255.\n",
        "\n",
        "\n",
        "\n",
        "      # Memasukkan segmentasi dan bounding box ke dalam file masks dan boxes\n",
        "      masks = np.array(masks)\n",
        "      boxes = self.get_boxes(masks)\n",
        "\n",
        "      # Create target dict\n",
        "      num_objs = len(masks)\n",
        "      # membuat ektraksi ciri\n",
        "      boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "      labels = torch.ones((num_objs,), dtype=torch.int64)\n",
        "      masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
        "      image = torch.as_tensor(image, dtype=torch.float32)\n",
        "      data = {}\n",
        "      data[\"boxes\"] =  boxes\n",
        "      data[\"labels\"] = labels\n",
        "      data[\"masks\"] = masks\n",
        "\n",
        "      return image, data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "ORQe6btAJH8d"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "  images = list()\n",
        "  targets = list()\n",
        "  for b in batch:\n",
        "        images.append(b[0])\n",
        "        targets.append(b[1])\n",
        "  images = torch.stack(images, dim=0)\n",
        "  return images, targets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "Rk-Ch2N-JJle"
      },
      "outputs": [],
      "source": [
        "import albumentations as A\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "y16ITsckJL3p"
      },
      "outputs": [],
      "source": [
        "transform = A.Compose([\n",
        "    # A.Resize(600, 600),\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.VerticalFlip(p=0.5),\n",
        "    A.RandomBrightnessContrast(\n",
        "        contrast_limit=0.2, brightness_limit=0.3, p=0.5),\n",
        "    A.OneOf([\n",
        "        A.ImageCompression(p=0.8),\n",
        "        A.RandomGamma(p=0.8),\n",
        "        A.Blur(p=0.8),\n",
        "        A.Equalize(mode='cv',p=0.8)\n",
        "    ], p=1.0),\n",
        "    A.OneOf([\n",
        "        A.ImageCompression(p=0.8),\n",
        "        A.RandomGamma(p=0.8),\n",
        "        A.Blur(p=0.8),\n",
        "        A.Equalize(mode='cv',p=0.8),\n",
        "    ], p=1.0)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "BzM1YHmbJOG1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "from torchvision.models.detection import maskrcnn_resnet50_fpn\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision import datasets, models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "61RAe5H5kwiC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4111048-76d0-4364-8911-c4be71351736"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.01s)\n",
            "creating index...\n",
            "index created!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: {'id': 0, 'name': 'Pest', 'supercategory': 'none'},\n",
              " 1: {'id': 1, 'name': 'Belalang', 'supercategory': 'Pest'},\n",
              " 2: {'id': 2, 'name': 'Sehat', 'supercategory': 'Pest'},\n",
              " 3: {'id': 3, 'name': 'Ulat', 'supercategory': 'Pest'},\n",
              " 4: {'id': 4, 'name': 'Wereng', 'supercategory': 'Pest'}}"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ],
      "source": [
        "# n_classes = 3\n",
        "coco = COCO(os.path.join( \"train\", \"_annotations.coco.json\"))\n",
        "categories = coco.cats\n",
        "n_classes = len(categories.keys())\n",
        "categories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "LsC0GJBZJP4w"
      },
      "outputs": [],
      "source": [
        "def get_model():\n",
        "    model = maskrcnn_resnet50_fpn(pretrained=True)\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(\n",
        "            in_features, n_classes)\n",
        "    model.to(DEVICE)\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "9q-PZWviJRh2"
      },
      "outputs": [],
      "source": [
        "def save_checkpoint(state, filename=\"mask_rcnn.pth.tar\"):\n",
        "    print(\"=> Saving checkpoint\")\n",
        "    torch.save(state, filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "0XR0-IG9JTZn"
      },
      "outputs": [],
      "source": [
        "def load_checkpoint(checkpoint, model, optimizer, lr):\n",
        "    print(\"=> Loading checkpoint\")\n",
        "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "    #optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
        "\n",
        "    # If we don't do this then it will just have learning rate of old checkpoint\n",
        "    # and it will lead to many hours of debugging \\:\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group[\"lr\"] = lr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "1a2lVUzdJU59"
      },
      "outputs": [],
      "source": [
        "if LOAD_MODEL and CHECKPOINT_FILE in os.listdir():\n",
        "        print(\"Loading checkpoint\")\n",
        "        load_checkpoint(torch.load(CHECKPOINT_FILE), model, optimizer, LEARNING_RATE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "QDvyHC3ZNCi2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d8ce0b0e-f96a-4d51-8504-618d5c4a134e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'train'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 107
        }
      ],
      "source": [
        "y = os.path.join( 'train')\n",
        "y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "Ixq0s3NmNK7M"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "SxNmiI5VJaAN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71067e29-7a78-4681-c325-b99ad58efd93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "loading annotations into memory...\n",
            "Done (t=0.01s)\n",
            "creating index...\n",
            "index created!\n"
          ]
        }
      ],
      "source": [
        "train_dataset = CCDataset(mode='train', augmentation=transform)\n",
        "train_loader = DataLoader(dataset=train_dataset,\n",
        "                              batch_size=4,\n",
        "                              shuffle=True,\n",
        "                              # drop_last=True,\n",
        "                              num_workers=4,\n",
        "                              pin_memory=PIN_MEMORY,\n",
        "                              collate_fn=collate_fn)\n",
        "\n",
        "valid_dataset = CCDataset(mode='valid')\n",
        "valid_loader = DataLoader(dataset=valid_dataset,\n",
        "                              batch_size=2,\n",
        "                              shuffle=False,\n",
        "                              # drop_last=True,\n",
        "                              pin_memory=PIN_MEMORY,\n",
        "                              collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.utils import draw_bounding_boxes"
      ],
      "metadata": {
        "id": "tCtWav6mVhiW"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "id": "0ejvRiokJdqT"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(loader, model, optimizer, device):\n",
        "    loop = tqdm(loader)\n",
        "\n",
        "    all_losses = []\n",
        "    all_losses_dict = []\n",
        "    lr_scheduler = None\n",
        "    if epoch == 0:\n",
        "        warmup_factor = 1.0 / 1000\n",
        "        warmup_iters = min(1000, len(loader) - 1)\n",
        "\n",
        "        lr_scheduler = torch.optim.lr_scheduler.LinearLR(\n",
        "            optimizer, start_factor=warmup_factor, total_iters=warmup_iters\n",
        "        )\n",
        "    for batch_idx, (images, targets) in  enumerate(loop):\n",
        "        images = list(image.to(device) for image in images)\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "        \n",
        "        loss_dict = model(images, targets)\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "        loss_dict_append = {k: v.item() for k, v in loss_dict.items()}\n",
        "        loss_value = losses.item()\n",
        "          \n",
        "        all_losses.append(loss_value)\n",
        "        all_losses_dict.append(loss_dict_append)\n",
        "          \n",
        "        if not math.isfinite(loss_value):\n",
        "            print(f\"Loss is {loss_value}, stopping trainig\") # train if loss becomes infinity\n",
        "            print(loss_dict)\n",
        "            sys.exit(1)\n",
        "\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    \n",
        "    return(np.mean(all_losses))\n",
        "    all_losses_dict = pd.DataFrame(all_losses_dict) # for printing\n",
        "    # print(\"Epoch {}, lr: {:.6f}, loss: {:.6f}, loss_classifier: {:.6f}, loss_box: {:.6f}, loss_rpn_box: {:.6f}, loss_object: {:.6f}\".format(\n",
        "    #     epoch, optimizer.param_groups[0]['lr'], np.mean(all_losses),\n",
        "    #     all_losses_dict['loss_classifier'].mean(),\n",
        "    #     all_losses_dict['loss_box_reg'].mean(),\n",
        "    #     all_losses_dict['loss_rpn_box_reg'].mean(),\n",
        "    #     all_losses_dict['loss_objectness'].mean()\n",
        "    # ))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def _get_iou_types(model):\n",
        "#     model_without_ddp = model\n",
        "#     if isinstance(model, torch.nn.parallel.DistributedDataParallel):\n",
        "#         model_without_ddp = model.module\n",
        "#     iou_types = [\"bbox\"]\n",
        "#     if isinstance(model_without_ddp, torchvision.models.detection.MaskRCNN):\n",
        "#         iou_types.append(\"segm\")\n",
        "#     if isinstance(model_without_ddp, torchvision.models.detection.KeypointRCNN):\n",
        "#         iou_types.append(\"keypoints\")\n",
        "#     return iou_types"
      ],
      "metadata": {
        "id": "FTJLeTTyUa7s"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def evaluate(model, loader, device):\n",
        "#     n_threads = torch.get_num_threads()\n",
        "#     # FIXME remove this and make paste_masks_in_image run on the GPU\n",
        "#     torch.set_num_threads(1)\n",
        "#     # cpu_device = torch.device(\"cpu\")\n",
        "#     model.eval()\n",
        "#     loop = tqdm(loader)\n",
        "#     header = \"Test:\"\n",
        "\n",
        "#     coco = get_coco_api_from_dataset(loop.dataset)\n",
        "#     iou_types = _get_iou_types(model)\n",
        "#     coco_evaluator = CocoEvaluator(coco, iou_types)\n",
        "\n",
        "#     for batch_idx, (images, targets) in  enumerate(loop):\n",
        "#         images = list(img.to(device) for img in images)\n",
        "\n",
        "#         if torch.cuda.is_available():\n",
        "#             torch.cuda.synchronize()\n",
        "#         model_time = time.time()\n",
        "#         outputs = model(images)\n",
        "\n",
        "#         outputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]\n",
        "#         model_time = time.time() - model_time\n",
        "\n",
        "#         res = {target[\"image_id\"].item(): output for target, output in zip(targets, outputs)}\n",
        "#         evaluator_time = time.time()\n",
        "#         coco_evaluator.update(res)\n",
        "#         evaluator_time = time.time() - evaluator_time\n",
        "#         metric_logger.update(model_time=model_time, evaluator_time=evaluator_time)\n",
        "\n",
        "#     # gather the stats from all processes\n",
        "#     metric_logger.synchronize_between_processes()\n",
        "#     print(\"Averaged stats:\", metric_logger)\n",
        "#     coco_evaluator.synchronize_between_processes()\n",
        "\n",
        "#     # accumulate predictions from all images\n",
        "#     coco_evaluator.accumulate()\n",
        "#     coco_evaluator.summarize()\n",
        "#     torch.set_num_threads(n_threads)\n",
        "#     return coco_evaluator"
      ],
      "metadata": {
        "id": "00fQbI2xUm4i"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "eXrbfSGcJgud"
      },
      "outputs": [],
      "source": [
        "best_vloss = np.inf\n",
        "def validate(loader, model, optimizer, device, epoch):\n",
        "    global best_vloss\n",
        "    loop = tqdm(loader)\n",
        "    running_vloss = 0\n",
        "    for batch_idx, (images, targets) in enumerate(loop):\n",
        "        images = list(image.to(device) for image in images)\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "        \n",
        "        with torch.no_grad():\n",
        "          loss_dict = model(images, targets)\n",
        "        \n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "        running_vloss += losses\n",
        "        \n",
        "    avg_vloss = running_vloss / (batch_idx + 1)\n",
        "    print(f\"Avg Valid Loss: {avg_vloss}\")\n",
        "    if avg_vloss < best_vloss:\n",
        "      best_vloss = avg_vloss\n",
        "      # if SAVE_MODEL:\n",
        "      #       print(\"Model improved, saving...\")\n",
        "      #       checkpoint = {\n",
        "      #           \"state_dict\": model.state_dict(),\n",
        "      #           \"optimizer\": optimizer.state_dict(),\n",
        "      #       }\n",
        "      #       save_checkpoint(checkpoint, filename=f\"1152KaggleBest_second_{epoch}.pth.tar\")\n",
        "    print('\\n')\n",
        "    return avg_vloss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "7j5wBKORJlxs"
      },
      "outputs": [],
      "source": [
        "model = get_model()\n",
        "optimizer = torch.optim.AdamW(params=model.parameters(),\n",
        "                                  lr=LEARNING_RATE,\n",
        "                                  weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "# params = [p for p in model.parameters() if p.requires_grad]\n",
        "# optimizer = torch.optim.SGD(params, lr=0.01, momentum=0.9, nesterov=True, weight_decay=1e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SsJPKvDkJnd9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff7ff02c-1ba5-4a98-cf44-b444080e6fef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 71%|███████   | 29/41 [00:26<00:10,  1.11it/s]"
          ]
        }
      ],
      "source": [
        "model.train()\n",
        "train_error =[]\n",
        "# val_error = []\n",
        "num_epochs = 25\n",
        "for epoch in range(num_epochs):\n",
        "                print(f\"Epoch: {epoch}\")\n",
        "                train = train_one_epoch(train_loader, model, optimizer, DEVICE)\n",
        "                # vloss= validate(valid_loader, model, optimizer, DEVICE, epoch)\n",
        "                train_error.append(train*100)\n",
        "                # val_error.append(vloss.tolist()*100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f3ycHgYArNuq"
      },
      "outputs": [],
      "source": [
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(train_error, label='Training Loss')\n",
        "# plt.plot(val_error, label='Validation Loss')\n",
        "plt.legend()\n",
        "plt.title('Training Loss')\n",
        "plt.axvline(num_epochs, color=\"gray\", label=\"Epoch\")\n",
        "# plt.ylabel('merror')\n",
        "plt.ylabel('Loss')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QhJ0Eptc0uVE"
      },
      "outputs": [],
      "source": [
        "# train"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print(\"Score\")\n",
        "# for i in range(2):\n",
        "#   img, _ = valid_dataset[i]\n",
        "#   img_int = torch.tensor(img*255, dtype=torch.uint8)\n",
        "#   with torch.no_grad():\n",
        "#       prediction = model([img.to(DEVICE)])\n",
        "#       pred = prediction[0]\n",
        "  \n",
        "#   print( [classes[i] for i in pred['labels'][pred['scores'] > 0.8].tolist()], \": \",pred['scores'].tolist() )\n",
        "#   fig = plt.figure(figsize=(14, 10))\n",
        "#   plt.imshow(draw_bounding_boxes(img_int,\n",
        "#     pred['boxes'][pred['scores'] > 0.8], \n",
        "#     [classes[i] for i in pred['labels'][pred['scores'] > 0.8].tolist()], width=4\n",
        "#   ).permute(1, 2, 0))\n",
        "\n",
        "  \n",
        "  \n",
        "    \n",
        "  \n"
      ],
      "metadata": {
        "id": "kM32bWr7ljGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ofgTXCR0wJX"
      },
      "outputs": [],
      "source": [
        "# vloss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZftiV-YvJtO4"
      },
      "outputs": [],
      "source": [
        "import cv2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wbsVeoKDES_H"
      },
      "outputs": [],
      "source": [
        "classes = [i[1]['name'] for i in categories.items()]\n",
        "classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6-vB9_0qbJOw"
      },
      "outputs": [],
      "source": [
        "def predict_single_frame(frame):\n",
        "    images = cv2.resize(frame, IMAGE_SIZE, cv2.INTER_LINEAR)/255\n",
        "    images = torch.as_tensor(images, dtype=torch.float32).unsqueeze(0)\n",
        "    images = images.swapaxes(1, 3).swapaxes(2, 3)\n",
        "    images = list(image.to(DEVICE) for image in images)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "      pred = model(images)\n",
        "    \n",
        "    print(pred)\n",
        "    \n",
        "    im = images[0].swapaxes(0, 2).swapaxes(0, 1).detach().cpu().numpy().astype(np.float32)\n",
        "    im2 = np.zeros_like(im).astype(np.float32)\n",
        "    for i in range(len(pred[0]['masks'])):\n",
        "        msk=pred[0]['masks'][i,0].detach().cpu().numpy()\n",
        "        scr=pred[0]['scores'][i].detach().cpu().numpy()\n",
        "        box=pred[0]['boxes'][i].detach().cpu().numpy()\n",
        "        lbl=pred[0]['labels'][i].detach().cpu().numpy()\n",
        "        if (lbl == 1) or (lbl == 3) or (lbl == 4) :\n",
        "          x = \"hama\"\n",
        "        else :\n",
        "          x = \"tidak hama\"\n",
        "        print(lbl)\n",
        "        if scr>0.9 :\n",
        "            cv2.rectangle(im, (int(box[0]), int(box[1])), (int(box[2]), int(box[3])), (0,0,1), 2)\n",
        "            cv2.putText(im, \"{0:.2f}%\".format(scr*100), (int(box[0]+10), int(box[1])+40), cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                        0.5, (0,0,1), 2, cv2.LINE_AA)\n",
        "            cv2.putText(im, x , (int(box[0]+4), int(box[1])+15), cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                        0.5, (0,0,1), 2, cv2.LINE_AA)\n",
        "            im2[:,:,0][msk>0.87] = np.random.uniform(0,1)\n",
        "            im2[:, :, 1][msk > 0.87] = np.random.uniform(0,1)\n",
        "            im2[:, :, 2][msk > 0.87] = np.random.uniform(0,1)\n",
        "\n",
        "    return (cv2.addWeighted(im, 0.8, im2, 0.2,0)*255).astype(np.uint8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KezIaX3WnTc2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJ6qsoa3bJ3x"
      },
      "outputs": [],
      "source": [
        "cap = cv2.VideoCapture('/content/drive/MyDrive/Ta/Mask_RCNN/Dataset/Uji/2023-01-22_12.17.14.272.png')\n",
        "model.train(False)\n",
        "\n",
        "if (cap.isOpened()== False): \n",
        "    print(\"Error opening video stream or file\")\n",
        "\n",
        "images = []   \n",
        "while(cap.isOpened()):\n",
        "    ret, frame = cap.read()\n",
        "    if ret == True:\n",
        "        result_frame = predict_single_frame(frame)\n",
        "        images.append(result_frame)\n",
        "    else: \n",
        "        break\n",
        "\n",
        "cap.release()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T1BTpNxFj0iq"
      },
      "outputs": [],
      "source": [
        "# result_frame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O8wKXtnXPpRZ"
      },
      "outputs": [],
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "cv2_imshow(result_frame)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-pbRdZPeLJtk"
      },
      "outputs": [],
      "source": [
        "cap = cv2.VideoCapture('/content/drive/MyDrive/Ta/Mask_RCNN/Dataset/Uji/2023-01-22_11.55.18.642.png')\n",
        "model.train(False)\n",
        "\n",
        "if (cap.isOpened()== False): \n",
        "    print(\"Error opening video stream or file\")\n",
        "\n",
        "images = []   \n",
        "while(cap.isOpened()):\n",
        "    ret, frame = cap.read()\n",
        "    if ret == True:\n",
        "        result_frame = predict_single_frame(frame)\n",
        "        images.append(result_frame)\n",
        "    else: \n",
        "        break\n",
        "\n",
        "cap.release()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XSDE5F9RLLsP"
      },
      "outputs": [],
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "cv2_imshow(result_frame)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t = torch.tensor([1,2,3]) # cara 1\n",
        "print(t)"
      ],
      "metadata": {
        "id": "0628JsiCu7FH"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNU9Rjcu7NAtH1MqH/DZXyp",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}